{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "corrected-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "from scipy import stats\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aware-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "hired-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from flask import  Flask, request, jsonify, render_template\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "known-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciamos nuestra API\n",
    "app = Flask('Emision de CO2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "maritime-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el modelo\n",
    "\n",
    "@app.route('/entrenar_modelo',methods=['POST'])\n",
    "def entrenar_modelo():\n",
    "    \n",
    "    # la función \"request.get_json\" de Flask para capturar la información que le envíemos a la API\n",
    "    data = request.get_json(force=True)\n",
    "    \n",
    "    # Separamos la información de data y usamos json.loads() para transformar el dataframe que está en formato\n",
    "    # json a un diccionario y luego lo convertimos en un DataFrame.\n",
    "    df=pd.DataFrame(json.loads(data['base']))\n",
    "    columns_name=data['lista_predictores']\n",
    "    target_name=data['target']\n",
    "    directorio_modelo=data['directory']\n",
    "    nombre_modelo=data['name']\n",
    "        \n",
    "    # Separamos las \"X\" y la \"y\" para entrenar nuestro modelo\n",
    "    X = df[columns_name]\n",
    "    y = df[target_name]\n",
    "    \n",
    "    # Hacemos el train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, random_state=40)\n",
    "    \n",
    "    # Instaciamos el modelo de regresion Lineal.\n",
    "    modelo = LinearRegression()\n",
    "    \n",
    "    # Entrenamos el modelo \n",
    "    modelo.fit(X_train, y_train)\n",
    "        \n",
    "    # Evaluamos el R2 de train y de test\n",
    "    result_train=r2_score(y_train,modelo.predict(X_train))\n",
    "    result_test=r2_score(y_test,modelo.predict(X_test))\n",
    "        \n",
    "    # Guardamos el modelo entrenado en una carpeta que se llama \"modelos\" que esté a la misma altura que la notebook\n",
    "    dir_=directorio_modelo+'/'+nombre_modelo+'.pkl'\n",
    "    with open(dir_, 'wb') as modelo_pkl:\n",
    "        pickle.dump(modelo, modelo_pkl)\n",
    "       \n",
    "    \n",
    "    # La función devuelva el alpha elegido por el modelo, el resultado de train y el de test.\n",
    "    return jsonify({'r2_resultado_train':float(result_train), 'r2_resultado_test':float(result_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-center",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciamos nuestro segunda función asociada a su endpoint. Usamos el método POST ya que vamos a enviar información\n",
    "# al servidor: la figura con los coeficientes de nuestro modelo entrenado\n",
    "@app.route('/plot_coeficientes',methods=['POST'])\n",
    "def plotear_coeficientes():\n",
    "    \n",
    "    # la función \"request.get_json\" de Flask para capturar la información que le envíemos a la API\n",
    "    data = request.get_json(force=True)\n",
    "    \n",
    "    # Separamos la información de data.\n",
    "    direccion=data['direccion']\n",
    "    name_modelo=data['nombre_modelo']\n",
    "    colores=data['paleta_colores']\n",
    "    size=data['tamano']\n",
    "    predictores=data['predictores']\n",
    "    \n",
    "    # Levantamos el modelo que tenemos grabado en disco con el llamado anterior\n",
    "    # (hay que tener en cuenta que tenemos que hacer el llamado anterior primero cuando estemos en la notebook\n",
    "    # que simula ser el cliente).\n",
    "    dir_input=direccion+'/'+name_modelo+'.pkl'\n",
    "    with open(dir_input, 'rb') as modelo_pkl:\n",
    "        modelo_load = pickle.load(modelo_pkl)\n",
    "        \n",
    "    # Levantamos los coeficientes ya calculados del modelo\n",
    "    # Los ponemos en un dataframe junto con el nombre de los predictores.\n",
    "    # Pasamos los coeficientes a valores absolutos y los ordenamos de mayor a \n",
    "    # menor para hacer el gráfico.\n",
    "    coef=abs(pd.DataFrame({'coeficientes':modelo_load.coef_},index=predictores)).\\\n",
    "                           sort_values(by='coeficientes',ascending=False)\n",
    "    \n",
    "    \n",
    "    # Generamos la figura y la guardamos\n",
    "    fig, ((ax1)) = plt.subplots(1,1,gridspec_kw={'hspace': 0.45, 'wspace': 0.15},figsize=size)\n",
    "    fig.suptitle(\"Coeficientes\",y=0.96,x=0.135,fontsize=24,fontweight='bold')\n",
    "\n",
    "    ax1 = sns.barplot(x=\"index\", y=\"coeficientes\", data=coef.reset_index(),ax=ax1,palette=colores)\n",
    "    ax1.xaxis.set_label_text('Coeficientes')\n",
    "    ax1.yaxis.set_label_text('Valores coeficientes')\n",
    "    plt.close(); #usamos esta línea para que la figura no se imprima en pantalla\n",
    "    \n",
    "    # guardamos la figura donde se guardan los modelos\n",
    "    dir_figura=direccion+'/'+name_modelo+'.jpg'\n",
    "    fig.savefig(dir_figura,dpi=150)\n",
    "    \n",
    "    # devolvemos un json con los coeficientes. Dado que las API devuelven la información en formato\n",
    "    # json, usamos el método de los dataframes \".to_json()\" para poder retornar los datos de los\n",
    "    # coeficientes.\n",
    "    return jsonify(coef.to_json())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "timely-kruger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"Emision de CO2\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [14/Mar/2021 09:42:37] \"\u001b[37mPOST /entrenar_modelo HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Mar/2021 09:42:42] \"\u001b[37mPOST /entrenar_modelo HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Ahora ejecutamos esta línea de código que pone a disposición los tres endpoints que armamos arriba. \n",
    "# Es hora de ir a la otra notebook en la que simulamos ser un cliente y hacer los llamadados para cada\n",
    "# uno de estos tres endopoints...\n",
    "app.run(host='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dado que en este endopint no vamos a guardar nada en el servidor, sino recibir información, usamos \n",
    "# el método GET. \n",
    "@app.route(\"/prediccion\",methods=['GET'])\n",
    "def predecir_puntaje():\n",
    "    \n",
    "    # Usamos request.args para tomar las query que le pasamos a la URL\n",
    "    direccion=request.args['direccion']\n",
    "    name_modelo=request.args['nombre_modelo']\n",
    "    name_scaler=request.args['nombre_scalador']\n",
    "    caso_to_predict=request.args['features']\n",
    "    \n",
    "    # Levantamos el modelo y el escalador que ya tenemos entrenado\n",
    "    with open(direccion+'/'+name_modelo+'.pkl', 'rb') as modelo_pkl:\n",
    "        modelo_load = pickle.load(modelo_pkl)\n",
    "        \n",
    "    with open(direccion+'/'+name_scaler+'.pkl', 'rb') as scaler_pkl:\n",
    "        scaler_load = pickle.load(scaler_pkl)\n",
    "        \n",
    "    # re-escalamos los datos con el escalador entrenado (tengan en cuenta que el escalador va a estar\n",
    "    # esperando una estructura como un DataFrame)\n",
    "    scaled_case=scaler_load.transform(pd.DataFrame(json.loads(caso_to_predict),index=[0]))\n",
    "    \n",
    "    # realizamos la prediccion\n",
    "    prediccion=modelo_load.predict(scaled_case)\n",
    "    \n",
    "    return jsonify({'prediccion':float(prediccion)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
